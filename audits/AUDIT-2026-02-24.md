# Hand-Written Parser Audit â€” 2026-02-24

Audit of Phase A hand-written parser (lexer + Pratt parser) introduced to replace
pest-based parsing for the circuit path. Covers `token.rs`, `error.rs`, `lexer.rs`,
`parser.rs` (950 new lines across 4 files).

Workspace state: 793 tests passing, 0 failures. Parser crate: 76 tests.

---

## Summary

| Severity | Count | Open | IDs |
|----------|-------|------|-----|
| Medium | 3 | 0 | ~~P-01~~, ~~P-02~~, ~~P-03~~ |
| Low | 3 | 0 | ~~P-04~~, ~~P-05~~, ~~P-06~~ |

**All 6 findings fixed.** 797 workspace tests passing.

---

## Medium (3)

### P-01: String escape processing divergence â€” FIXED

- **Files**: `lexer.rs:286-316` vs `build_ast.rs:403-408`
- **Issue**: The new lexer processes escape sequences inline during tokenization
  (`\n` â†’ real newline, `\t` â†’ real tab, etc.), storing the **interpreted value** in
  `Token.lexeme`. The pest parser stores the **raw source text** (`\n` as literal
  backslash + `n`) because the `inner = @{ char* }` rule is atomic.
- **Resolution**: Lexer now validates escapes but stores raw text in `Token.lexeme`.
  Added `pub fn unescape(raw: &str) -> String` utility (re-exported from crate root).
  Both parsers now produce identical `StringLit` values. Also resolves P-06.

### P-02: Prove source extraction uses fragile line/col â†’ byte offset conversion â€” FIXED

- **Files**: `parser.rs:797-829` (`find_prove_offset`, `find_source_end`)
- **Issue**: The `Expr::Prove.source` field requires capturing the original source text
  from `prove` to `}`. The new parser reconstructed byte offsets from `(line, col)` via
  `find_prove_offset()`, which counted **characters** per column while the lexer's `col`
  increments per **byte**. Multi-byte UTF-8 in same-line block comments before `prove`
  caused a mismatch.
- **Resolution**: Added `byte_offset: usize` field to `Token` (set from `self.pos` in
  the lexer). `parse_prove()` now uses `token.byte_offset` directly to slice source text.
  Deleted `find_prove_offset()` and `find_source_end()` entirely (~32 lines removed).

### P-03: Comparison chaining is more permissive than pest grammar â€” FIXED

- **Files**: `parser.rs:704` vs `grammar.pest:154`
- **Issue**: The Pratt parser allowed arbitrary comparison chaining (`a < b < c`), while
  pest's grammar intentionally limits to one comparison per expression level to prevent
  silent bugs.
- **Resolution**: After parsing a comparison operator, if the next token is also a
  comparison operator, the parser emits: _"comparison operators cannot be chained; use
  `&&` to combine: `a < b && b < c`"_. Tested with `<`, `==`, `>=`/`<=` combinations.

---

## Low (3)

### P-04: Dead code branch in `parse_fn_expr` â€” FIXED

- **File**: `parser.rs:649-660`
- **Issue**: Two branches with identical bodies; the `lookahead(1) == LParen` check in
  branch A was redundant since `expect(LParen)` 3 lines later catches the error.
- **Resolution**: Simplified to single `if self.at(&TokenKind::Ident)` branch.

### P-05: Lexer truncates multi-byte UTF-8 characters in strings â€” FIXED

- **File**: `lexer.rs:310-312`
- **Issue**: The lexer cast each byte to `char` independently inside strings, corrupting
  multi-byte UTF-8 characters (e.g., `Ã©` â†’ `ÃƒÂ©`).
- **Resolution**: String lexing now decodes full UTF-8 characters via `std::str::from_utf8`
  on the remaining source slice, advancing by the correct byte length. Tested with 2-byte
  (`cafÃ©`), 3-byte (`ä¸–ç•Œ`), and 4-byte (`ðŸŽ‰`) characters.

### P-06: Token.lexeme semantic inconsistency â€” FIXED

- **File**: `lexer.rs:234,271,316`
- **Issue**: `StringLit` tokens stored escape-processed content while all other tokens
  stored raw source text.
- **Resolution**: Resolved by P-01 fix. `Token.lexeme` now consistently contains raw
  source text for all token types.

---

## Verified Correct (no findings)

| Area | Verification |
|------|-------------|
| Operator precedence (8 levels) | Golden tests confirm identical AST structure vs pest for all precedence levels |
| Right-associative `^` | `2^3^4` â†’ `Pow(2, Pow(3,4))` â€” verified identical in both parsers |
| Prefix `-`/`!` vs `^` ordering | `-a^2` â†’ `Neg(Pow(a,2))` â€” prefix BP 11, `^` BP (12,11) correct |
| Map vs block disambiguation | LL-3 lookahead `{ ident/str : â†’ map` works correctly, `{}` is block |
| Assignment detection | Parse expr, then check for `=` â€” matches pest's ordered-choice behavior |
| `fn_decl` vs `fn_expr` | Statement-level `fn name(` â†’ FnDecl, all else â†’ expression â€” identical |
| `for` range detection | `integer .. integer` via lookahead, falls back to expression â€” identical |
| `public`/`witness` array syntax | `name[N]` with comma-separated lists â€” golden test verified |
| Negative numbers | Both parsers produce `UnaryOp(Neg, Number("7"))` â€” empirically verified |
| Keyword guard | Lexer keyword table rejects `let`, `if`, etc. as identifiers correctly |
| Trailing commas | Arrays, maps, call args all handled â€” matches pest `","?` grammar |
| Line + block comments | `//` to EOL and `/* */` both handled, including nested in skip loop |
| Error messages | Significantly improved over pest: `expected ), found +` vs `expected EOI` |

---

## Fix Status

| ID | Fix | Status |
|------|------|--------|
| P-01 | Raw string storage + `unescape()` utility | **DONE** |
| P-02 | `byte_offset` on Token, eliminated `find_prove_offset` | **DONE** |
| P-03 | Comparison chaining rejection in Pratt loop | **DONE** |
| P-04 | Deleted redundant branch in `parse_fn_expr` | **DONE** |
| P-05 | UTF-8 char decoding in lexer strings | **DONE** |
| P-06 | Resolved by P-01 | **DONE** |
